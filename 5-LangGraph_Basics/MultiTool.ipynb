{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d3aa9d",
   "metadata": {},
   "source": [
    "# MultiTool Caller Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097fb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623afaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=5, doc_content_chars_max=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820abe5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2025-03-30\\nTitle: TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning\\nAuthors: Jing Zhu, Xiang Song, Vassilis N. Ioannidis, Danai Koutra, Christos Faloutsos\\nSummary: How can we enhance the node features acquired from Pretrained Models (PMs) to\\nbetter suit downstream graph learning tasks? Graph Neural Networks (GNNs) have\\nbecome the state-of-the-art approach for many high-impact, real-world graph\\napplications. For feature-rich graphs, a prevalent practice invol'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.invoke(\"Multi-Modal Audio Visual Language Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e57c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)\n",
    "Wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "print(Wiki.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ce401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: Rick and Morty\\nSummary: Rick and Morty is an American adult animated science fiction sitcom created by Justin Roiland and Dan Harmon for Cartoon Network's nighttime programming block Adult Swim. The series follows the misadventures of Rick Sanchez, a cynical mad scientist, and his good-hearted but fretful grandson Morty Smith, who split their time between domestic life and interdimensional adventures that take place across an infinite number of realities, often traveling to other planets a\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wiki.invoke(\"Rick and Morty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2958e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d37cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/7vccwwq12psb87cv4d66wzn00000gn/T/ipykernel_6434/735679136.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily = TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625647e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': \"Run OpenAI's new GPT-OSS (open-source) model on Northflank\",\n",
       "  'url': 'https://northflank.com/blog/self-host-openai-gpt-oss-120b-open-source-chatgpt',\n",
       "  'content': \"Unlike GPT-3.5 or GPT-4, which are closed-source and API-only, GPT-OSS is available to run locally or in your own infrastructure. That means full control over latency, cost, and privacy, especially when paired with a secure, GPU-ready platform like Northflank.\\n\\nSelf-hosting gpt-oss also means you won’t run into any rate limits. According to OpenAI, gpt-oss 120B’s performance is on par with o4-mini on most benchmarks, which would make it one of the top open-source models, if not the best. [...] Run OpenAI's new GPT-OSS (open-source) model on Northflank\\n==========================================================\\n\\nAI\\n\\nOpenAI just released GPT-OSS, its first fully open-source large language model family under an Apache 2.0 license. The release includes two models: gpt-oss-20b and gpt-oss-120b, designed for fast, low-latency inference with strong reasoning and instruction-following capabilities. [...] Northflank makes it simple to deploy and run these models in a secure, high-performance environment.\\n\\nWith our one-click deploy template, you can get started in minutes, without any infrastructure setup required.\\n\\nTL;DR\\n\\n   OpenAI released GPT-OSS, a powerful open-source LLM family under Apache 2.0.\\n   The 120B model delivers top-tier performance and runs smoothly on 2×H100.\\n   You can deploy it in minutes on Northflank using our one-click stack with vLLM + Open WebUI. No rate limits.\",\n",
       "  'score': 0.86441374},\n",
       " {'title': 'Welcome GPT OSS, the new open-source model family from OpenAI!',\n",
       "  'url': 'https://huggingface.co/blog/welcome-openai-gpt-oss',\n",
       "  'content': 'thought on the final message each time. [...] > We aim for our tools to be used safely, responsibly, and democratically, while maximizing your control over how you use them. By using gpt-oss, you agree to comply with all applicable law. [...] , and a smaller one with 21B parameters (gpt-oss-20b). Both are mixture-of-experts (MoEs) and use a 4-bit quantization scheme (MXFP4), enabling fast inference (thanks to fewer active parameters, see details below) while keeping resource usage low. The large model fits on a single H100 GPU, while the small one runs within 16GB of memory and is perfect for consumer hardware and on-device applications.',\n",
       "  'score': 0.81133276},\n",
       " {'title': 'Introducing gpt-oss | OpenAI',\n",
       "  'url': 'https://openai.com/index/introducing-gpt-oss/',\n",
       "  'content': 'Pre-training & model architecture\\n---------------------------------\\n\\nThe gpt-oss models were trained using our most advanced pre-training and post-training techniques, with particular focus on reasoning, efficiency, and real-world usability across a wide range of deployment environments. While we have made other models including Whisper\\u2060 and CLIP\\u2060 available openly, gpt-oss models are our first open-weight language models since GPT‑2. [...] For developers who want fully customizable models they can fine-tune and deploy in their own environments, gpt-oss is a great fit. For those seeking multimodal support, built-in tools, and seamless integration with our platform, models available through our API platform remain the best option. We’re continuing to listen closely to developer feedback and may consider API support for gpt-oss in the future. [...] We’re releasing gpt-oss-120b and gpt-oss-20b—two state-of-the-art open-weight language models that deliver strong real-world performance at low cost. Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks, demonstrate strong tool use capabilities, and are optimized for efficient deployment on consumer hardware. They were trained using a mix of reinforcement learning and techniques informed by OpenAI’s most advanced internal',\n",
       "  'score': 0.7944651},\n",
       " {'title': 'openai/gpt-oss: gpt-oss-120b and gpt-oss-20b are two open-weight ...',\n",
       "  'url': 'https://github.com/openai/gpt-oss',\n",
       "  'content': \"Repository files navigation\\n---------------------------\\n\\n   README\\n   Apache-2.0 license\\n\\nImage 2: gpt-oss-120\\n\\nTry gpt-oss · Guides · Model card · OpenAI blog\\n\\nDownload gpt-oss-120b and gpt-oss-20b on Hugging Face\\n\\nWelcome to the gpt-oss series, OpenAI's open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.\\n\\nWe're releasing two flavors of these open models: [...] GitHub - openai/gpt-oss: gpt-oss-120b and gpt-oss-20b are two open-weight language models by OpenAI\\n\\n===============\\n\\nSkip to content\\nNavigation Menu\\n---------------\\n\\nToggle navigation [...] The reference implementations in this repository are meant as a starting point and inspiration. Outside of bug fixes we do not intend to accept new feature contributions. If you build implementations based on this code such as new tool implementations you are welcome to contribute them to the `awesome-gpt-oss.md` file.\\n\\nAbout\\n-----\\n\\ngpt-oss-120b and gpt-oss-20b are two open-weight language models by OpenAI\\n\\nopenai.com/open-models\\n\\n### Resources\\n\\nReadme\\n\\n### License\\n\\nApache-2.0 license\",\n",
       "  'score': 0.7602419},\n",
       " {'title': 'OpenAI GPT OSS 20b API & Playground - Fireworks AI',\n",
       "  'url': 'https://fireworks.ai/models/fireworks/gpt-oss-20b',\n",
       "  'content': \"gpt oss 20b is a compact, open-weight language model optimized for low-latency and resource-constrained environments, including local and edge deployments. It shares the same Harmony training foundation and capabilities as 120B, with faster inference and easier deployment that is ideal for specialized or offline use cases, fast responsive performance, chain-of-thought output and adjustable reasoning levels, and agentic workflows.\\n\\n## Fireworks Features\\n\\n### Serverless [...] OpenAI gpt oss 120b & 20b, open weight models designed for reasoning, agentic tasks, and versatile developer use cases is now available! Try Now\\n\\nFireworks Logo\\nOpenAi Logo MArk\\n\\n# OpenAI gpt oss 20b [...] gpt oss 20b is available via Fireworks' serverless API, where you pay per token. There are several ways to call the Fireworks API, including Fireworks' Python client, the REST API, or OpenAI's Python client.\\n\\n### On-demand Deployment\\n\\nOn-demand deployments allow you to use gpt oss 20b on dedicated GPUs with Fireworks' high-performance serving stack with high reliability and no rate limits.\\n\\n## Info\\n\\n### Provider\\n\\nOpenAI\\n\\n### Model Type\\n\\n### Context Length\\n\\n128K\\n\\n### Serverless\\n\\nAvailable\",\n",
       "  'score': 0.6786044}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"What is GPT-OSS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c8edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[arxiv, Wiki, tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf4ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"Gemma2-9b-It\")\n",
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e48d91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'wikipedia',\n",
       "  'args': {'query': 'WACV'},\n",
       "  'id': 'w4cksnpx3',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "llm_with_tools.invoke([HumanMessage(content=f\"What is WACV?\"),]).tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ef260",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c09683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KN_Agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
